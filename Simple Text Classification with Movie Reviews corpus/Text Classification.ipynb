{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the NLTK library\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imporiting the relevant libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing  tokenizers\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the movie reviews corpus\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Overview of the corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the file IDs of the corpus\n",
    "# movie_reviews.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check the categories of the reviews\n",
    "movie_reviews.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'plot : two teen couples go to a church party , drink and then drive . \\nthey get into an accident . \\none of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . \\nwhat\\'s the deal ? \\nwatch the movie and \" sorta \" find out . . . \\ncritique : a mind-fuck movie for the teen generation that touches on a very cool idea , but presents it in a very bad package . \\nwhich is what makes this review an even harder one to write , since i generally applaud films which attempt to break the mold , mess with your head and such ( lost highway & memento ) , but there are good and bad ways of making all types of films , and these folks just didn\\'t snag this one correctly . \\nthey seem to have taken this pretty neat concept , but executed it terribly . \\nso what are the problems with the movie ? \\nwell , its main problem is that it\\'s simply too jumbled . \\nit starts off \" normal \" but then downshifts into this \" fantasy \" world in which you , as an audience member , have no idea what\\'s going on . \\nthere are dreams , there are characters coming back from the dead , there are others who look like the dead , there are strange apparitions , there are disappearances , there are a looooot of chase scenes , there are tons of weird things that happen , and most of it is simply not explained . \\nnow i personally don\\'t mind trying to unravel a film every now and then , but when all it does is give me the same clue over and over again , i get kind of fed up after a while , which is this film\\'s biggest problem . \\nit\\'s obviously got this big secret to hide , but it seems to want to hide it completely until its final five minutes . \\nand do they make things entertaining , thrilling or even engaging , in the meantime ? \\nnot really . \\nthe sad part is that the arrow and i both dig on flicks like this , so we actually figured most of it out by the half-way point , so all of the strangeness after that did start to make a little bit of sense , but it still didn\\'t the make the film all that more entertaining . \\ni guess the bottom line with movies like this is that you should always make sure that the audience is \" into it \" even before they are given the secret password to enter your world of understanding . \\ni mean , showing melissa sagemiller running away from visions for about 20 minutes throughout the movie is just plain lazy ! ! \\nokay , we get it . . . there \\nare people chasing her and we don\\'t know who they are . \\ndo we really need to see it over and over again ? \\nhow about giving us different scenes offering further insight into all of the strangeness going down in the movie ? \\napparently , the studio took this film away from its director and chopped it up themselves , and it shows . \\nthere might\\'ve been a pretty decent teen mind-fuck movie in here somewhere , but i guess \" the suits \" decided that turning it into a music video with little edge , would make more sense . \\nthe actors are pretty good for the most part , although wes bentley just seemed to be playing the exact same character that he did in american beauty , only in a new neighborhood . \\nbut my biggest kudos go out to sagemiller , who holds her own throughout the entire film , and actually has you feeling her character\\'s unraveling . \\noverall , the film doesn\\'t stick because it doesn\\'t entertain , it\\'s confusing , it rarely excites and it feels pretty redundant for most of its runtime , despite a pretty cool ending and explanation to all of the craziness that came before it . \\noh , and by the way , this is not a horror or teen slasher flick . . . it\\'s \\njust packaged to look that way because someone is apparently assuming that the genre is still hot with the kids . \\nit also wrapped production two years ago and has been sitting on the shelves ever since . \\nwhatever . . . skip \\nit ! \\nwhere\\'s joblo coming from ? \\na nightmare of elm street 3 ( 7/10 ) - blair witch 2 ( 7/10 ) - the crow ( 9/10 ) - the crow : salvation ( 4/10 ) - lost highway ( 10/10 ) - memento ( 10/10 ) - the others ( 9/10 ) - stir of echoes ( 8/10 ) \\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check the first file ID in raw text \n",
    "first_review = movie_reviews.raw('neg/cv000_29416.txt')\n",
    "first_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#total negative reviews : 1000\n",
      "#total positive reviews : 1000\n"
     ]
    }
   ],
   "source": [
    "# let's check the lengths of positive and negative reviews \n",
    "print(\"#total negative reviews :\", len(movie_reviews.fileids('neg')))\n",
    "print(\"#total positive reviews :\", len(movie_reviews.fileids('pos')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list which will contain the file IDs and its correspodning categories\n",
    "documents = []\n",
    "for category in movie_reviews.categories():\n",
    "    for fileid in movie_reviews.fileids(category):\n",
    "        documents.append((movie_reviews.words(fileid), category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['plot', ':', 'two', 'teen', 'couples', 'go', 'to', ...], 'neg')\n"
     ]
    }
   ],
   "source": [
    "# let's have a look at our documents list\n",
    "print(documents[0])         # the first element of the documents list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling the documents list\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Feature Extraction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To classify the text into any category, we need to convert the word into features. So, this will help in training our classifier.\n",
    "\n",
    "In our first case, we will be using the **top-N words feature**.\n",
    "\n",
    "1. So, with this approach, we will be using the top 2000 most frequent words.\n",
    "\n",
    "2. We'll create a feature set which will contain the top-N words in a boollean form and the categories of the reviews.\n",
    "\n",
    "3. We have already shuffled the documents list which contain all the 2000 review texts - both positive and negative reviews. From these, we will be taking the top 2000 most common words and will create the feature set.\n",
    "\n",
    "4. After that we will be segregating the data to training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1583820"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print out the total number of word tokens in the reviews corpus\n",
    "len(movie_reviews.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a list of all word tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party']\n"
     ]
    }
   ],
   "source": [
    "# First, we need to take all the words present in the corpus and store them in a list\n",
    "all_words_tokens = [token.lower() for token in movie_reviews.words()]\n",
    "\n",
    "# Print out the first 10 words\n",
    "print(all_words_tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequency Distribution of all the word tokens\n",
    "\n",
    "--> This will calculate the number if occurrence of each word in the entire list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the FreqDist (Frequency Distribution) from nltk\n",
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 39768 samples and 1583820 outcomes>\n"
     ]
    }
   ],
   "source": [
    "# Calculating the freq dist of the all the word tokens\n",
    "all_words_tokens_freq = FreqDist(all_words_tokens)\n",
    "print(all_words_tokens_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have **39768 unique** word tokens out of the total word tokens i.e. 1583820."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 77717),\n",
       " ('the', 76529),\n",
       " ('.', 65876),\n",
       " ('a', 38106),\n",
       " ('and', 35576),\n",
       " ('of', 34123),\n",
       " ('to', 31937),\n",
       " (\"'\", 30585),\n",
       " ('is', 25195),\n",
       " ('in', 21822),\n",
       " ('s', 18513),\n",
       " ('\"', 17612),\n",
       " ('it', 16107),\n",
       " ('that', 15924),\n",
       " ('-', 15595)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's have a look for the top 15 most common word tokens (15 most frequently occurring word tokens)\n",
    "all_words_tokens_freq.most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see some punctuations and stopwords in the data. And most frequently occurring words are either punction marks or stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Stopwords and Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing stopwords and punctuations\n",
    "from nltk.corpus import stopwords\n",
    "import string               # for punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# Getting the English stopwords\n",
    "stopwords_eng = stopwords.words(\"english\")\n",
    "print(stopwords_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to remove stopwords and punctuations\n",
    "def remove_punc_stopwords(txt):\n",
    "    \"\"\"\n",
    "        1. First we will remove punctutations\n",
    "        2. Then, we will remove stopwords\n",
    "        3. Lastly, we will return the clean word tokens\n",
    "    \"\"\"\n",
    "    nopunc = [char for char in txt if char not in string.punctuation]\n",
    "    no_stops = [word for word in nopunc if word.lower() not in stopwords_eng]\n",
    "    return no_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the function on all word tokens\n",
    "all_words_tokens_cleaned = remove_punc_stopwords(all_words_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original len of all word tokens =  1583820\n",
      "After removal of stopwords and punctuations,  len of all word tokens =  710578\n"
     ]
    }
   ],
   "source": [
    "# Let's see the lengths of the all word tokens list prior and after removing stopwords and punctuations\n",
    "print(\"Original len of all word tokens = \", len(all_words_tokens))\n",
    "print(\"After removal of stopwords and punctuations,  len of all word tokens = \", len(all_words_tokens_cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Frequency Distribution of all the word tokens after removing punctuations and stopwords\n",
    "all_words_tokens_cleaned_freq = FreqDist(all_words_tokens_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('film', 9517),\n",
       " ('one', 5852),\n",
       " ('movie', 5771),\n",
       " ('like', 3690),\n",
       " ('even', 2565),\n",
       " ('good', 2411),\n",
       " ('time', 2411),\n",
       " ('story', 2169),\n",
       " ('would', 2109),\n",
       " ('much', 2049),\n",
       " ('character', 2020),\n",
       " ('also', 1967),\n",
       " ('get', 1949),\n",
       " ('two', 1911),\n",
       " ('well', 1906)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's see the top 15 most common words \n",
    "all_words_tokens_cleaned_freq.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 39768 samples and 1583820 outcomes>\n",
      "<FreqDist with 39586 samples and 710578 outcomes>\n"
     ]
    }
   ],
   "source": [
    "print(all_words_tokens_freq)\n",
    "print(all_words_tokens_cleaned_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4486482049727873"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "710578/1583820 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now we can see that the frequently occurring word tokens are not stopwords and punctuations. It got changed to some more meaningful word tokens when we print the top 15 most common words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to create word feature using 2000 most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39586\n"
     ]
    }
   ],
   "source": [
    "# Let's check the length of all using words\n",
    "print(len(all_words_tokens_cleaned_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*2000/40000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using around 5% of the most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('film', 9517), ('one', 5852), ('movie', 5771), ('like', 3690), ('even', 2565), ('good', 2411), ('time', 2411), ('story', 2169), ('would', 2109), ('much', 2049)]\n"
     ]
    }
   ],
   "source": [
    "# Most common words (2000 freq words)\n",
    "most_common_word_tokens = all_words_tokens_cleaned_freq.most_common(2000)\n",
    "\n",
    "# print top 10 most common words\n",
    "print(most_common_word_tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('remain', 64), ('anna', 64), ('moved', 64), ('asking', 64), ('genuinely', 64), ('rain', 64), ('path', 64), ('aware', 64), ('causes', 64), ('international', 64)]\n"
     ]
    }
   ],
   "source": [
    "# Least 10 freq words (botton 10 common words)\n",
    "print(most_common_word_tokens[1990:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['film', 'one', 'movie', 'like', 'even', 'good', 'time', 'story', 'would', 'much']\n"
     ]
    }
   ],
   "source": [
    "# Since the elements of the most_common_word_tokens list are in the form of tuples, we need to extract the first element of each tuple to get the words as word features\n",
    "word_features = [token[0] for token in most_common_word_tokens]\n",
    "\n",
    "# Print out the top 10 word features\n",
    "print(word_features[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Length of the word_features list\n",
    "len(word_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Feature Set\n",
    "\n",
    "--> We'll create a function to get the word features as a set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to get the features (words) in a dictionary\n",
    "def doc_features(doc):\n",
    "    \n",
    "    # creating a set for all the unique words present in a document\n",
    "    doc_words = set(doc)\n",
    "    \n",
    "    # creating an empty features list\n",
    "    features = {}\n",
    "    \n",
    "    # Will iterate through all the words present in the word_features list\n",
    "    for word in word_features:\n",
    "        \n",
    "        # Get that word and see its presence in the document (will return a bollean value)\n",
    "        features[word] = (word in doc_words)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos/cv000_29590.txt'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the file ID of the first positive review\n",
    "movie_reviews.fileids('pos')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the function on the first positive review\n",
    "# print(doc_features(movie_reviews.words('pos/cv000_29590.txt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we are going to create a feature set which will contain the word features of the review and its correspoding category\n",
    "feature_sets = [(doc_features(review), category) for (review, category) in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will create training and testing sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set and Testing set\n",
    "train_data = feature_sets[:1600]\n",
    "test_data = feature_sets[1600:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1600"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Length of training set\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Length of testing set\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8, 0.2)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1600/2000, 400/2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training our model i.e. classifier.\n",
    "\n",
    "We will be using the **Naive Bayes Classifier**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Base Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the NaiveBayesClassifier from nltk\n",
    "from nltk import NaiveBayesClassifier\n",
    "\n",
    "# Creating an instance of our classifier and training the model\n",
    "base_model = NaiveBayesClassifier.train(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing our base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score of Base Model : 75.75%\n"
     ]
    }
   ],
   "source": [
    "# Importing classify from nltk\n",
    "from nltk import classify\n",
    "\n",
    "# Calculating the accuracy of the base model \n",
    "accuracy_score = classify.accuracy(base_model, test_data)\n",
    "print(\"Accuracy Score of Base Model : {}%\".format(100 * accuracy_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             outstanding = True              pos : neg    =     20.5 : 1.0\n",
      "                   damon = True              pos : neg    =     15.3 : 1.0\n",
      "                   mulan = True              pos : neg    =      7.5 : 1.0\n",
      "             wonderfully = True              pos : neg    =      7.0 : 1.0\n",
      "                  seagal = True              neg : pos    =      6.8 : 1.0\n",
      "                    lame = True              neg : pos    =      6.2 : 1.0\n",
      "                 unfunny = True              neg : pos    =      5.6 : 1.0\n",
      "                  wasted = True              neg : pos    =      5.2 : 1.0\n",
      "                   waste = True              neg : pos    =      5.2 : 1.0\n",
      "                   flynt = True              pos : neg    =      4.9 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Show 10 most informative features\n",
    "print(base_model.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above result basically gives the **likelihood ratios** of the 10 most common. \n",
    "\n",
    "It shows that the word **\"outstanding\"** is used in positive reviews **20.5** times more often than it is used in negative reviews. The word **\"lame\"** is used in negative reviwes **6.8** times more than it's used in positive reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, we can have an idea about our **base_model**'s classification. A review will have a higher chance of getting a tag as positive if it contains words like **outstanding** and **wonderfully**. Likewise, a review has a higher chance of getting a negative tag if it contains words like **lame**, **wasted**, **waste**, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our base model gives result to some new reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new review\n",
    "new_review = \"I hated the movie. It was a disaster.Poor direction and bad acting\"\n",
    "\n",
    "# Creating word tokens\n",
    "new_review_tokens = word_tokenize(new_review)\n",
    "\n",
    "# Creating the word feature set\n",
    "new_review_set = doc_features(new_review_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg\n"
     ]
    }
   ],
   "source": [
    "# let's test the classifier on the custom review\n",
    "print(base_model.classify(new_review_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can see that the output comes out as **negative**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum proba of getting the above classification = neg\n",
      "Proba of getting a negative tag for the given review = 0.9999998929539884\n",
      "Proba of getting a positive tag for the given review = 1.0704602403014413e-07\n"
     ]
    }
   ],
   "source": [
    "# Let's see the probability of getting the above result\n",
    "prob = base_model.prob_classify(new_review_set)\n",
    "print(\"Maximum proba of getting the above classification =\", prob.max())\n",
    "print(\"Proba of getting a negative tag for the given review =\", prob.prob('neg'))\n",
    "print(\"Proba of getting a positive tag for the given review =\", prob.prob('pos'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take another custom review\n",
    "new_review_1 = \"It was an awesome movie. The direction was perfect. I loved it.\"\n",
    "\n",
    "# Getting the word tokens\n",
    "new_review_1_tokens = word_tokenize(new_review_1)\n",
    "\n",
    "# Getting the feature set\n",
    "new_review_1_set = doc_features(new_review_1_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg\n"
     ]
    }
   ],
   "source": [
    "# let's test the classifier on the custom review\n",
    "print(base_model.classify(new_review_1_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum proba of getting the above classification = neg\n",
      "Proba of getting a negative tag for the given review = 0.9999978215051157\n",
      "Proba of getting a positive tag for the given review = 2.17849489320483e-06\n"
     ]
    }
   ],
   "source": [
    "# Let's see the probability of getting the above result\n",
    "prob_1 = base_model.prob_classify(new_review_1_set)\n",
    "print(\"Maximum proba of getting the above classification =\", prob_1.max())\n",
    "print(\"Proba of getting a negative tag for the given review =\", prob_1.prob('neg'))\n",
    "print(\"Proba of getting a positive tag for the given review =\", prob_1.prob('pos'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, our **base_model** is not able to classify a positive review correctly. We have used the **top-N words** feature in our **base_model**. While creating the training and testing sets, we can have imbalance classes of positive and negative reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Bag-of-Words Approach**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will be using a **Bag-of-words** feature for our model. We'll be using **unigrams** (item having a single word i.e. the N-gram of size 1. E.g. bad.  So, it's just a token with only one word and we will unigramns features).\n",
    "\n",
    "1. In this approach, while creating the feature set we will be using all the useful words of each review. We will basically create two lists which will contain words for each review category (one for postive reviews and another for negative reviews).\n",
    "\n",
    "2. Then, we will create a function (bag-of-words) which will clean the word tokens for each review texts. Here, we will remove punctuations and stopwords and the function will return a dictionary of cleaned words. We, will use a dictionary because the dictionary will not take duplicate words and thus will oncly contain unique word tokens.\n",
    "\n",
    "3. After that, we will be using the bag-of-words function for positive reviews and negative reviews lists alongwith that, it will assign the positive and negative categories for the concerned reviews.\n",
    "\n",
    "4. Now, we will be taking a fixed number of positive and negative reviews for both the training and testing sets.\n",
    "\n",
    "5. So, this will ensure a balanced classes (categories) of the reviews for our model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the word tokens lists for both **positive** and **negative** reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, will create two list for postive and negative reviews word tokens\n",
    "\n",
    "# Positive reviews word tokens list\n",
    "pos_word_tokens = []\n",
    "for fileid in movie_reviews.fileids('pos'):\n",
    "    word_tokens = movie_reviews.words(fileid)\n",
    "    pos_word_tokens.append(word_tokens)\n",
    "\n",
    "# Negative reviews word tokens list\n",
    "neg_word_tokens = []\n",
    "for fileid in movie_reviews.fileids('neg'):\n",
    "    word_tokens = movie_reviews.words(fileid)\n",
    "    neg_word_tokens.append(word_tokens)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['films', 'adapted', 'from', 'comic', 'books', 'have', 'had', 'plenty', 'of', 'success', ',', 'whether', 'they', \"'\", 're', 'about', 'superheroes', '(', 'batman', ',', 'superman', ',', 'spawn', ')', ',', 'or', 'geared', 'toward', 'kids', '(']\n"
     ]
    }
   ],
   "source": [
    "# Print out the first pos review of pos_word_tokens\n",
    "print(pos_word_tokens[0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an', 'accident', '.', 'one', 'of', 'the', 'guys', 'dies', ',', 'but', 'his']\n"
     ]
    }
   ],
   "source": [
    "# Print out the first neg review of pos_word_tokens\n",
    "print(neg_word_tokens[0][:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Bag-of-Words** function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the bag-of-words function which will return a dictionary of cleanded words\n",
    "def bag_of_words(tokens):\n",
    "\n",
    "    # Using the remove_punc_stopwords function to remove the stopwords and punctuations\n",
    "    clean_word_tokens = remove_punc_stopwords(tokens)\n",
    "\n",
    "    # storing the cleaned words in a dictionary\n",
    "    words_dict = dict([char, True] for char in clean_word_tokens)\n",
    "    return words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'good': True, 'bad': True, 'poor': True, 'love': True, 'awesome': True}\n"
     ]
    }
   ],
   "source": [
    "# Let's check our bag-of-words function\n",
    "print(bag_of_words(['the', 'the', 'a', 'good', 'bad', 'the', 'poor', 'love', 'awesome']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Feature Sets for both positive and negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive reviews feature set\n",
    "pos_feature_set = []\n",
    "for tokens in pos_word_tokens:\n",
    "    pos_feature_set.append((bag_of_words(tokens), 'pos'))\n",
    "\n",
    "# Negative reviews feature set\n",
    "neg_feature_set = []\n",
    "for tokens in neg_word_tokens:\n",
    "    neg_feature_set.append((bag_of_words(tokens), 'neg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'films': True, 'adapted': True, 'comic': True, 'books': True, 'plenty': True, 'success': True, 'whether': True, 'superheroes': True, 'batman': True, 'superman': True, 'spawn': True, 'geared': True, 'toward': True, 'kids': True, 'casper': True, 'arthouse': True, 'crowd': True, 'ghost': True, 'world': True, 'never': True, 'really': True, 'book': True, 'like': True, 'hell': True, 'starters': True, 'created': True, 'alan': True, 'moore': True, 'eddie': True, 'campbell': True, 'brought': True, 'medium': True, 'whole': True, 'new': True, 'level': True, 'mid': True, '80s': True, '12': True, 'part': True, 'series': True, 'called': True, 'watchmen': True, 'say': True, 'thoroughly': True, 'researched': True, 'subject': True, 'jack': True, 'ripper': True, 'would': True, 'saying': True, 'michael': True, 'jackson': True, 'starting': True, 'look': True, 'little': True, 'odd': True, 'graphic': True, 'novel': True, '500': True, 'pages': True, 'long': True, 'includes': True, 'nearly': True, '30': True, 'consist': True, 'nothing': True, 'footnotes': True, 'words': True, 'dismiss': True, 'film': True, 'source': True, 'get': True, 'past': True, 'thing': True, 'might': True, 'find': True, 'another': True, 'stumbling': True, 'block': True, 'directors': True, 'albert': True, 'allen': True, 'hughes': True, 'getting': True, 'brothers': True, 'direct': True, 'seems': True, 'almost': True, 'ludicrous': True, 'casting': True, 'carrot': True, 'top': True, 'well': True, 'anything': True, 'riddle': True, 'better': True, 'set': True, 'ghetto': True, 'features': True, 'violent': True, 'street': True, 'crime': True, 'mad': True, 'geniuses': True, 'behind': True, 'menace': True, 'ii': True, 'society': True, 'question': True, 'course': True, 'whitechapel': True, '1888': True, 'london': True, 'east': True, 'end': True, 'filthy': True, 'sooty': True, 'place': True, 'whores': True, 'unfortunates': True, 'nervous': True, 'mysterious': True, 'psychopath': True, 'carving': True, 'profession': True, 'surgical': True, 'precision': True, 'first': True, 'stiff': True, 'turns': True, 'copper': True, 'peter': True, 'godley': True, 'robbie': True, 'coltrane': True, 'enough': True, 'calls': True, 'inspector': True, 'frederick': True, 'abberline': True, 'johnny': True, 'depp': True, 'blow': True, 'crack': True, 'case': True, 'widower': True, 'prophetic': True, 'dreams': True, 'unsuccessfully': True, 'tries': True, 'quell': True, 'copious': True, 'amounts': True, 'absinthe': True, 'opium': True, 'upon': True, 'arriving': True, 'befriends': True, 'unfortunate': True, 'named': True, 'mary': True, 'kelly': True, 'heather': True, 'graham': True, 'proceeds': True, 'investigate': True, 'horribly': True, 'gruesome': True, 'crimes': True, 'even': True, 'police': True, 'surgeon': True, 'stomach': True, 'think': True, 'anyone': True, 'needs': True, 'briefed': True, 'go': True, 'particulars': True, 'unique': True, 'interesting': True, 'theory': True, 'identity': True, 'killer': True, 'reasons': True, 'chooses': True, 'slay': True, 'bother': True, 'cloaking': True, 'screenwriters': True, 'terry': True, 'hayes': True, 'vertical': True, 'limit': True, 'rafael': True, 'yglesias': True, 'les': True, 'mis': True, 'rables': True, 'good': True, 'job': True, 'keeping': True, 'hidden': True, 'viewers': True, 'funny': True, 'watch': True, 'locals': True, 'blindly': True, 'point': True, 'finger': True, 'blame': True, 'jews': True, 'indians': True, 'englishman': True, 'could': True, 'capable': True, 'committing': True, 'ghastly': True, 'acts': True, 'ending': True, 'whistling': True, 'stonecutters': True, 'song': True, 'simpsons': True, 'days': True, 'holds': True, 'back': True, 'electric': True, 'car': True, 'made': True, 'steve': True, 'guttenberg': True, 'star': True, 'worry': True, 'make': True, 'sense': True, 'see': True, 'onto': True, 'appearance': True, 'certainly': True, 'dark': True, 'bleak': True, 'surprising': True, 'much': True, 'looks': True, 'tim': True, 'burton': True, 'planet': True, 'apes': True, 'times': True, 'sleepy': True, 'hollow': True, '2': True, 'print': True, 'saw': True, 'completely': True, 'finished': True, 'color': True, 'music': True, 'finalized': True, 'comments': True, 'marilyn': True, 'manson': True, 'cinematographer': True, 'deming': True, 'word': True, 'ably': True, 'captures': True, 'dreariness': True, 'victorian': True, 'era': True, 'helped': True, 'flashy': True, 'killing': True, 'scenes': True, 'remind': True, 'crazy': True, 'flashbacks': True, 'twin': True, 'peaks': True, 'though': True, 'violence': True, 'pales': True, 'comparison': True, 'black': True, 'white': True, 'oscar': True, 'winner': True, 'martin': True, 'childs': True, 'shakespeare': True, 'love': True, 'production': True, 'design': True, 'original': True, 'prague': True, 'surroundings': True, 'one': True, 'creepy': True, 'acting': True, 'solid': True, 'dreamy': True, 'turning': True, 'typically': True, 'strong': True, 'performance': True, 'deftly': True, 'handling': True, 'british': True, 'accent': True, 'ians': True, 'holm': True, 'joe': True, 'gould': True, 'secret': True, 'richardson': True, '102': True, 'dalmatians': True, 'log': True, 'great': True, 'supporting': True, 'roles': True, 'big': True, 'surprise': True, 'cringed': True, 'time': True, 'opened': True, 'mouth': True, 'imagining': True, 'attempt': True, 'irish': True, 'actually': True, 'half': True, 'bad': True, 'however': True, '00': True, 'r': True, 'gore': True, 'sexuality': True, 'language': True, 'drug': True, 'content': True}, 'pos')\n"
     ]
    }
   ],
   "source": [
    "# Printing out the first element of the pos feature set\n",
    "print(pos_feature_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'plot': True, 'two': True, 'teen': True, 'couples': True, 'go': True, 'church': True, 'party': True, 'drink': True, 'drive': True, 'get': True, 'accident': True, 'one': True, 'guys': True, 'dies': True, 'girlfriend': True, 'continues': True, 'see': True, 'life': True, 'nightmares': True, 'deal': True, 'watch': True, 'movie': True, 'sorta': True, 'find': True, 'critique': True, 'mind': True, 'fuck': True, 'generation': True, 'touches': True, 'cool': True, 'idea': True, 'presents': True, 'bad': True, 'package': True, 'makes': True, 'review': True, 'even': True, 'harder': True, 'write': True, 'since': True, 'generally': True, 'applaud': True, 'films': True, 'attempt': True, 'break': True, 'mold': True, 'mess': True, 'head': True, 'lost': True, 'highway': True, 'memento': True, 'good': True, 'ways': True, 'making': True, 'types': True, 'folks': True, 'snag': True, 'correctly': True, 'seem': True, 'taken': True, 'pretty': True, 'neat': True, 'concept': True, 'executed': True, 'terribly': True, 'problems': True, 'well': True, 'main': True, 'problem': True, 'simply': True, 'jumbled': True, 'starts': True, 'normal': True, 'downshifts': True, 'fantasy': True, 'world': True, 'audience': True, 'member': True, 'going': True, 'dreams': True, 'characters': True, 'coming': True, 'back': True, 'dead': True, 'others': True, 'look': True, 'like': True, 'strange': True, 'apparitions': True, 'disappearances': True, 'looooot': True, 'chase': True, 'scenes': True, 'tons': True, 'weird': True, 'things': True, 'happen': True, 'explained': True, 'personally': True, 'trying': True, 'unravel': True, 'film': True, 'every': True, 'give': True, 'clue': True, 'kind': True, 'fed': True, 'biggest': True, 'obviously': True, 'got': True, 'big': True, 'secret': True, 'hide': True, 'seems': True, 'want': True, 'completely': True, 'final': True, 'five': True, 'minutes': True, 'make': True, 'entertaining': True, 'thrilling': True, 'engaging': True, 'meantime': True, 'really': True, 'sad': True, 'part': True, 'arrow': True, 'dig': True, 'flicks': True, 'actually': True, 'figured': True, 'half': True, 'way': True, 'point': True, 'strangeness': True, 'start': True, 'little': True, 'bit': True, 'sense': True, 'still': True, 'guess': True, 'bottom': True, 'line': True, 'movies': True, 'always': True, 'sure': True, 'given': True, 'password': True, 'enter': True, 'understanding': True, 'mean': True, 'showing': True, 'melissa': True, 'sagemiller': True, 'running': True, 'away': True, 'visions': True, '20': True, 'throughout': True, 'plain': True, 'lazy': True, 'okay': True, 'people': True, 'chasing': True, 'know': True, 'need': True, 'giving': True, 'us': True, 'different': True, 'offering': True, 'insight': True, 'apparently': True, 'studio': True, 'took': True, 'director': True, 'chopped': True, 'shows': True, 'might': True, 'decent': True, 'somewhere': True, 'suits': True, 'decided': True, 'turning': True, 'music': True, 'video': True, 'edge': True, 'would': True, 'actors': True, 'although': True, 'wes': True, 'bentley': True, 'seemed': True, 'playing': True, 'exact': True, 'character': True, 'american': True, 'beauty': True, 'new': True, 'neighborhood': True, 'kudos': True, 'holds': True, 'entire': True, 'feeling': True, 'unraveling': True, 'overall': True, 'stick': True, 'entertain': True, 'confusing': True, 'rarely': True, 'excites': True, 'feels': True, 'redundant': True, 'runtime': True, 'despite': True, 'ending': True, 'explanation': True, 'craziness': True, 'came': True, 'oh': True, 'horror': True, 'slasher': True, 'flick': True, 'packaged': True, 'someone': True, 'assuming': True, 'genre': True, 'hot': True, 'kids': True, 'also': True, 'wrapped': True, 'production': True, 'years': True, 'ago': True, 'sitting': True, 'shelves': True, 'ever': True, 'whatever': True, 'skip': True, 'joblo': True, 'nightmare': True, 'elm': True, 'street': True, '3': True, '7': True, '10': True, 'blair': True, 'witch': True, '2': True, 'crow': True, '9': True, 'salvation': True, '4': True, 'stir': True, 'echoes': True, '8': True}, 'neg')\n"
     ]
    }
   ],
   "source": [
    "# Printing out the first element of the neg feature set\n",
    "print(neg_feature_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the pos feature set : 1000\n",
      "Length of the neg feature set : 1000\n"
     ]
    }
   ],
   "source": [
    "# Lengths of the feature sets\n",
    "print(\"Length of the pos feature set :\", len(pos_feature_set))\n",
    "print(\"Length of the neg feature set :\", len(neg_feature_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling the feature sets\n",
    "random.shuffle(pos_feature_set)\n",
    "random.shuffle(neg_feature_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **New Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating training set and testing set by taking 20% of pos reviews and 20% of neg reviews for the testing set\n",
    "# and remaining will be for the training set\n",
    "test_data1 = pos_feature_set[:200] + neg_feature_set[:200]\n",
    "train_data1 = pos_feature_set[200:] + neg_feature_set[200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the test_data1 : 400\n",
      "Length of the train_data1 : 1600\n"
     ]
    }
   ],
   "source": [
    "# Printing out the lengths of the training and testing datasets\n",
    "print(\"Length of the test_data1 :\", len(test_data1))\n",
    "print(\"Length of the train_data1 :\", len(train_data1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training our datasets with **NaiveBayesClassifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an instance of NaiveBayesClassifier and training it\n",
    "model1 = NaiveBayesClassifier.train(train_data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score of Model1 : 73.75%\n"
     ]
    }
   ],
   "source": [
    "# Calculating the accuracy of the model1 \n",
    "accuracy_score1 = classify.accuracy(model1, test_data1)\n",
    "print(\"Accuracy Score of Model1 : {}%\".format(100 * accuracy_score1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             outstanding = True              pos : neg    =     12.3 : 1.0\n",
      "                  avoids = True              pos : neg    =     12.3 : 1.0\n",
      "                chilling = True              pos : neg    =     12.3 : 1.0\n",
      "                 idiotic = True              neg : pos    =     12.2 : 1.0\n",
      "            effortlessly = True              pos : neg    =     11.7 : 1.0\n",
      "               ludicrous = True              neg : pos    =     11.4 : 1.0\n",
      "               stupidity = True              neg : pos    =     11.0 : 1.0\n",
      "                gripping = True              pos : neg    =     11.0 : 1.0\n",
      "                    taxi = True              pos : neg    =     10.3 : 1.0\n",
      "                  turkey = True              neg : pos    =     10.2 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Show 10 most informative features\n",
    "print(model1.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new review\n",
    "new_review = \"I hated the movie. It was a disaster.Poor direction and bad acting\"\n",
    "\n",
    "# Creating word tokens\n",
    "new_review_tokens = word_tokenize(new_review)\n",
    "\n",
    "# Creating the word feature set\n",
    "new_review_set = doc_features(new_review_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg\n"
     ]
    }
   ],
   "source": [
    "# let's test the classifier on the custom review\n",
    "print(model1.classify(new_review_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum proba of getting the above classification = neg\n",
      "Proba of getting a negative tag for the given review = 0.6632450528397239\n",
      "Proba of getting a positive tag for the given review = 0.33675494716114834\n"
     ]
    }
   ],
   "source": [
    "# Let's see the probability of getting the above result\n",
    "prob_new = model1.prob_classify(new_review_set)\n",
    "print(\"Maximum proba of getting the above classification =\", prob_new.max())\n",
    "print(\"Proba of getting a negative tag for the given review =\", prob_new.prob('neg'))\n",
    "print(\"Proba of getting a positive tag for the given review =\", prob_new.prob('pos'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take another custom review\n",
    "new_review_1 = \"It was an awesome movie. The direction was perfect. I loved it.\"\n",
    "\n",
    "# Getting the word tokens\n",
    "new_review_1_tokens = word_tokenize(new_review_1)\n",
    "\n",
    "# Getting the feature set\n",
    "new_review_1_set = doc_features(new_review_1_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos\n"
     ]
    }
   ],
   "source": [
    "# let's test the classifier on the custom review\n",
    "print(model1.classify(new_review_1_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum proba of getting the above classification = pos\n",
      "Proba of getting a negative tag for the given review = 0.16160922817930654\n",
      "Proba of getting a positive tag for the given review = 0.8383907718198518\n"
     ]
    }
   ],
   "source": [
    "# Let's see the probability of getting the above result\n",
    "prob_new_1 = model1.prob_classify(new_review_1_set)\n",
    "print(\"Maximum proba of getting the above classification =\", prob_new_1.max())\n",
    "print(\"Proba of getting a negative tag for the given review =\", prob_new_1.prob('neg'))\n",
    "print(\"Proba of getting a positive tag for the given review =\", prob_new_1.prob('pos'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the above model - **model1** is much better than the **base_model**. The model1 has correctly classified both the custom reviews. This shows an improvement in our modelling by using the Bag-of-words approach and using balanced datasets. Next, we will try using **Bi-grams** features for our model.\n",
    "\n",
    "**Bi-gram** - Item having two words, i.e. the N-gram of size 2. E.g. \"Very bad\".\n",
    "\n",
    "Similary, we also have **Tri-Grams** (trigrams) - Item having three words, i.e. The N-gram of size 3. E.g. \"Not very bad\".\n",
    "\n",
    "All these grams are known as **N-grams**. N-grams are continuous sequences of N words or symbols or tokens in a document. E.g “Medium blog” is a 2-gram (a bigram), “A Medium blog post” is a 4-gram, and “Write on Medium” is a 3-gram (trigram)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will be defining 3 functions:\n",
    "1. For removing punctuations and stopwords.\n",
    "2. For getting only unigram features.\n",
    "3. For getting the bigram fetaures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the ngrams \n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We already created for clean words\n",
    "# remove_punc_stopwords()\n",
    "# But we will be modifying it later for getting bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a bag-of-words func for extracting only the unigram features\n",
    "def bag_of_unigrams(tokens):\n",
    "    words_dict_uni = dict([token, True] for token in tokens)\n",
    "    return words_dict_uni\n",
    "\n",
    "\n",
    "# Creating a bag-of-words fucn for extracting for bigrams\n",
    "def bag_of_ngrams(tokens, n=2):\n",
    "    \n",
    "    # A list for storing the bigrams\n",
    "    words_ngrams = []\n",
    "\n",
    "    # Iterating through items that being created as bigrams and appending them in the words_ngrams\n",
    "    for item in iter(ngrams(tokens, n)):\n",
    "        words_ngrams.append(item)\n",
    "\n",
    "    # Will create a dictionary to store the unique bigram words\n",
    "    words_dict_bi = dict([token, True] for token in words_ngrams)\n",
    "    return words_dict_bi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample word tokens of the given text : ['The', 'movie', 'was', 'amazing', '.', 'Brilliantly', 'played', 'all', 'roles', 'by', 'all', 'the', 'actors', '.', 'It', 'was', 'very', 'good', '.'] \n",
      "\n",
      "Cleaned Words : ['movie', 'amazing', 'Brilliantly', 'played', 'roles', 'actors', 'good'] \n",
      "\n",
      "Unigrams : {'movie': True, 'amazing': True, 'Brilliantly': True, 'played': True, 'roles': True, 'actors': True, 'good': True} \n",
      "\n",
      "Bigrams : {('movie', 'amazing'): True, ('amazing', 'Brilliantly'): True, ('Brilliantly', 'played'): True, ('played', 'roles'): True, ('roles', 'actors'): True, ('actors', 'good'): True}\n"
     ]
    }
   ],
   "source": [
    "# Let's check our above functions on sample texts\n",
    "txt = \"The movie was amazing. Brilliantly played all roles by all the actors. It was very good.\"\n",
    "word_tokens_sample = word_tokenize(txt)\n",
    "print(\"Sample word tokens of the given text :\", word_tokens_sample, '\\n')\n",
    "\n",
    "# First removing puncs and stopwords\n",
    "cleaned_words_sample = remove_punc_stopwords(word_tokens_sample)\n",
    "print(\"Cleaned Words :\", cleaned_words_sample, '\\n')\n",
    "\n",
    "# Making unigrams\n",
    "print(\"Unigrams :\", bag_of_unigrams(cleaned_words_sample), '\\n')\n",
    "\n",
    "# Making bigrams\n",
    "print(\"Bigrams :\", bag_of_ngrams(cleaned_words_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaned word tokens are fine for unigrams but **cleaning words can be a disadvantage for bigrams because the cleaning process can omit important word tokens for bigrams**.\n",
    "\n",
    "E.g. stopwords like over, under, so, very, etc. are important for bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('The', 'movie'): True, ('movie', 'was'): True, ('was', 'amazing'): True, ('amazing', '.'): True, ('.', 'Brilliantly'): True, ('Brilliantly', 'played'): True, ('played', 'all'): True, ('all', 'roles'): True, ('roles', 'by'): True, ('by', 'all'): True, ('all', 'the'): True, ('the', 'actors'): True, ('actors', '.'): True, ('.', 'It'): True, ('It', 'was'): True, ('was', 'very'): True, ('very', 'good'): True, ('good', '.'): True}\n"
     ]
    }
   ],
   "source": [
    "# Let's use the uncleaned sample word tokens for getting bigrams\n",
    "print(bag_of_ngrams(word_tokens_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can create a new stopwords list especially for bigrams by subtracting important words from the whole set of stopwords\n",
    "important_words = ['above', 'below', 'off', 'over', 'under', 'more', 'most', 'such', 'no', 'nor', 'not', 'only', 'so', 'than', 'too', 'very', 'just', 'but']\n",
    "\n",
    "stopwords_eng_for_bigrams = set(stopwords_eng) - set(important_words)\n",
    "stopwords_eng_for_bigrams = list(stopwords_eng_for_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"doesn't\", 'you', 'until', 'as', 'further', 'll', 'which', \"you're\", 'having', 'all', \"it's\", 'being', 'be', 'when', \"mightn't\", 'our', 'other', 'yourself', 'doing', 're', 'both', \"didn't\", 'doesn', 'into', 'those', 'yours', 'this', 've', 'there', 'now', 'or', 'whom', 'with', 'from', 'has', 'myself', \"she's\", \"aren't\", 'hadn', 'in', 'had', 'have', \"hasn't\", 'their', 'haven', 'aren', 'him', 'against', 'he', 'them', 'and', 'needn', 'each', 'me', 'who', 'shan', 'between', 'didn', 'can', \"that'll\", \"weren't\", 'ourselves', 'about', 'won', 'these', \"needn't\", 'your', 'will', 'theirs', 'because', 'wasn', 'been', 'ma', 'm', 'then', 'ours', 'for', 'before', 'during', 'wouldn', 'the', 'are', 's', 'd', 'of', \"shouldn't\", 'don', \"isn't\", 'at', 'his', 'few', 'we', 'while', 'does', \"hadn't\", 'himself', 'any', 'after', 'should', 'do', 'hasn', 'an', 'here', 'that', 'again', 'am', 'own', \"won't\", 'what', 'hers', 'itself', \"haven't\", 'yourselves', 'shouldn', 'her', 'was', \"you've\", 'same', 'she', 'where', 'how', 'my', 'to', 'isn', 'it', 'i', 'mustn', \"you'd\", 'o', 'on', 'some', \"shan't\", \"should've\", 'is', 'themselves', \"don't\", 'herself', 'if', 'mightn', \"wasn't\", 't', \"you'll\", 'up', 'out', \"wouldn't\", 'its', 'a', 'ain', 'were', 'through', \"couldn't\", 'down', 'weren', 'couldn', 'by', \"mustn't\", 'y', 'once', 'did', 'why', 'they']\n"
     ]
    }
   ],
   "source": [
    "# Printing out the stopwords for bigrams\n",
    "print(stopwords_eng_for_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to remove stopwords and punctuations for bigrams\n",
    "def remove_punc_stopwords_for_bigrams(txt):\n",
    "    \"\"\"\n",
    "        1. First we will remove punctutations\n",
    "        2. Then, we will remove stopwords for bigrams\n",
    "        3. Lastly, we will return the clean word tokens\n",
    "    \"\"\"\n",
    "    nopunc = [char for char in txt if char not in string.punctuation]\n",
    "    no_stops = [word for word in nopunc if word.lower() not in stopwords_eng_for_bigrams]\n",
    "    return no_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Words : ['movie', 'amazing', 'Brilliantly', 'played', 'roles', 'actors', 'good'] \n",
      "\n",
      "Cleaned Words for bigrams: ['movie', 'amazing', 'Brilliantly', 'played', 'roles', 'actors', 'very', 'good'] \n",
      "\n",
      "Unigrams : {'movie': True, 'amazing': True, 'Brilliantly': True, 'played': True, 'roles': True, 'actors': True, 'good': True} \n",
      "\n",
      "Bigrams : {('movie', 'amazing'): True, ('amazing', 'Brilliantly'): True, ('Brilliantly', 'played'): True, ('played', 'roles'): True, ('roles', 'actors'): True, ('actors', 'very'): True, ('very', 'good'): True}\n"
     ]
    }
   ],
   "source": [
    "# First removing puncs and stopwords\n",
    "cleaned_words_sample = remove_punc_stopwords(word_tokens_sample)\n",
    "cleaned_words_sample_for_bigrams = remove_punc_stopwords_for_bigrams(word_tokens_sample)\n",
    "print(\"Cleaned Words :\", cleaned_words_sample, '\\n')\n",
    "print(\"Cleaned Words for bigrams:\", cleaned_words_sample_for_bigrams, '\\n')\n",
    "\n",
    "# Making unigrams\n",
    "print(\"Unigrams :\", bag_of_unigrams(cleaned_words_sample), '\\n')\n",
    "\n",
    "# Making bigrams\n",
    "print(\"Bigrams :\", bag_of_ngrams(cleaned_words_sample_for_bigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movie': True, 'amazing': True, 'Brilliantly': True, 'played': True, 'roles': True, 'actors': True, 'good': True, ('movie', 'amazing'): True, ('amazing', 'Brilliantly'): True, ('Brilliantly', 'played'): True, ('played', 'roles'): True, ('roles', 'actors'): True, ('actors', 'very'): True, ('very', 'good'): True}\n"
     ]
    }
   ],
   "source": [
    "# Now, let's combine unigram and bigram features of the sample text\n",
    "unigram_feats = bag_of_unigrams(cleaned_words_sample)\n",
    "bigram_feats = bag_of_ngrams(cleaned_words_sample_for_bigrams)\n",
    "\n",
    "all_features = unigram_feats.copy()\n",
    "all_features.update(bigram_feats)\n",
    "print(all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a function that extracts all features - unigrams and bigrams combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function extract all features\n",
    "def bag_of_all_words(tokens, n=2):\n",
    "    clean_word_tokens = remove_punc_stopwords(tokens)\n",
    "    clean_word_tokens_bigrams = remove_punc_stopwords_for_bigrams(tokens)\n",
    "\n",
    "    unigram_feats = bag_of_unigrams(clean_word_tokens)\n",
    "    bigram_feats = bag_of_ngrams(clean_word_tokens_bigrams)\n",
    "\n",
    "    all_features = unigram_feats.copy()\n",
    "    all_features.update(bigram_feats)\n",
    "\n",
    "    return all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movie': True, 'amazing': True, 'Brilliantly': True, 'played': True, 'roles': True, 'actors': True, 'good': True, ('movie', 'amazing'): True, ('amazing', 'Brilliantly'): True, ('Brilliantly', 'played'): True, ('played', 'roles'): True, ('roles', 'actors'): True, ('actors', 'very'): True, ('very', 'good'): True}\n"
     ]
    }
   ],
   "source": [
    "# Let's print out the bag of all words for the sample text\n",
    "print(bag_of_all_words(word_tokens_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the pos_word_tokens and neg_word_tokens that we have already created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Creating Feature Sets for both postive and negative reviews\n",
    "pos_feature_set_new = []\n",
    "for tokens in pos_word_tokens:\n",
    "    pos_feature_set_new.append((bag_of_all_words(tokens), 'pos'))\n",
    "\n",
    "neg_feature_set_new = []\n",
    "for tokens in neg_word_tokens:\n",
    "    neg_feature_set_new.append((bag_of_all_words(tokens), 'neg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling the feature sets\n",
    "random.shuffle(pos_feature_set_new)\n",
    "random.shuffle(neg_feature_set_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **New Model Training with both Unigrams and Bigrams combined**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating training set and testing set by taking 20% of pos reviews and 20% of neg reviews for the testing set\n",
    "# and remaining will be for the training set\n",
    "test_data2 = pos_feature_set_new[:200] + neg_feature_set_new[:200]\n",
    "train_data2 = pos_feature_set_new[200:] + neg_feature_set_new[200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the test_data2 : 400\n",
      "Length of the train_data2 : 1600\n"
     ]
    }
   ],
   "source": [
    "# Printing out the lengths of the training and testing datasets\n",
    "print(\"Length of the test_data2 :\", len(test_data2))\n",
    "print(\"Length of the train_data2 :\", len(train_data2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training our dataset with **NaiveBayesClassifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an instance of NaiveBayesClassifier and training it\n",
    "model2 = NaiveBayesClassifier.train(train_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score of Model2 : 82.0%\n"
     ]
    }
   ],
   "source": [
    "# Calculating the accuracy of the model1 \n",
    "accuracy_score2 = classify.accuracy(model2, test_data2)\n",
    "print(\"Accuracy Score of Model2 : {}%\".format(100 * accuracy_score2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new review\n",
    "new_review = \"I hated the movie. It was a disaster.Poor direction and bad acting\"\n",
    "\n",
    "# Creating word tokens\n",
    "new_review_tokens = word_tokenize(new_review)\n",
    "\n",
    "# Creating the word feature set\n",
    "new_review_set = doc_features(new_review_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg\n"
     ]
    }
   ],
   "source": [
    "# let's test the classifier on the custom review\n",
    "print(model2.classify(new_review_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum proba of getting the above classification = neg\n",
      "Proba of getting a negative tag for the given review = 0.6553117522052161\n",
      "Proba of getting a positive tag for the given review = 0.34468824779534163\n"
     ]
    }
   ],
   "source": [
    "# Let's see the probability of getting the above result\n",
    "prob_new_model2 = model2.prob_classify(new_review_set)\n",
    "print(\"Maximum proba of getting the above classification =\", prob_new_model2.max())\n",
    "print(\"Proba of getting a negative tag for the given review =\", prob_new_model2.prob('neg'))\n",
    "print(\"Proba of getting a positive tag for the given review =\", prob_new_model2.prob('pos'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take another custom review\n",
    "new_review_1 = \"It was an awesome movie. The direction was perfect. I loved it.\"\n",
    "\n",
    "# Getting the word tokens\n",
    "new_review_1_tokens = word_tokenize(new_review_1)\n",
    "\n",
    "# Getting the feature set\n",
    "new_review_1_set = doc_features(new_review_1_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos\n"
     ]
    }
   ],
   "source": [
    "# let's test the classifier on the custom review\n",
    "print(model2.classify(new_review_1_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum proba of getting the above classification = pos\n",
      "Proba of getting a negative tag for the given review = 0.1375131044241187\n",
      "Proba of getting a positive tag for the given review = 0.8624868955751197\n"
     ]
    }
   ],
   "source": [
    "# Let's see the probability of getting the above result\n",
    "prob_new_2_model2 = model2.prob_classify(new_review_1_set)\n",
    "print(\"Maximum proba of getting the above classification =\", prob_new_2_model2.max())\n",
    "print(\"Proba of getting a negative tag for the given review =\", prob_new_2_model2.prob('neg'))\n",
    "print(\"Proba of getting a positive tag for the given review =\", prob_new_2_model2.prob('pos'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results - accuracy scores of **model1** and **model2**, we can conclude that **model2** is an improvement over the **model1**.\n",
    "\n",
    "**model1 accuracy = 73.75%   (having only unigram features)**\n",
    "\n",
    "**model2 accuracy = 82.0%  (having both unigram and bigram features)**\n",
    "\n",
    "So, combining both unigrams and bigrams will be (sometimes) a better option in NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: I haven't displayed the result and code for show_most_informative_features, because I'm getting some errors in it. It's mostly related to unsupported oprations between datatypes and I'm not able to come around with solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "51168439cf5d01886d72eed480c6978dddaafbd160b3925976544b19dd1aa31c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
